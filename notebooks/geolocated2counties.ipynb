{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code sorts the raw tweets with the computed sentiment scores into county-level files based on their FIPS codes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"/Users/felix/ETH/Shared/100/Geolocated\"\n",
    "out_dir = \"/Users/felix/ETH/Shared/100/Counties_Felix\"\n",
    "\n",
    "in_flist = os.listdir(in_dir)\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(in_flist)\n",
    "\n",
    "# to store a summary of number of tweets dropped due to missing geolocation \n",
    "missing_geolocation = {}\n",
    "\n",
    "# here, on the very top we would have a loop over the raw files:\n",
    "for fname in in_flist:\n",
    "    print(\"\\n--------------- Processing file {} ---------------\".format(fname))\n",
    "\n",
    "    # read file, auto-convert dtypes and set date time index\n",
    "    df = pd.read_pickle(os.path.join(in_dir, fname))\n",
    "    df = df.infer_objects()\n",
    "    df = df.set_index(\"Date\")\n",
    "\n",
    "    # TODO: It seems that the geolocation script did not locate all tweets. \n",
    "    # have to deal with those somehow. Drop them for now to experiment further.\n",
    "    df_cleaned = df.drop(df.loc[df.FIPS == \"\"].index, axis=0)\n",
    "    \n",
    "    # count missing tweets\n",
    "    missing_geolocation[fname] = [df.shape[0], \n",
    "                                  df_cleaned.shape[0], \n",
    "                                  df.shape[0] - df_cleaned.shape[0], \n",
    "                                  1 - (df_cleaned.shape[0] / df.shape[0])]\n",
    "    \n",
    "    print(\"Dropped {} tweets due to missing geolocation.\\n\".format(df.shape[0] - df_cleaned.shape[0]))\n",
    "\n",
    "    # get unique fips codes within the raw file\n",
    "    fips_codes = df_cleaned.FIPS.unique()\n",
    "\n",
    "    # iterate over fips codes and store the corresponding tweets in seperate, county-level data frames\n",
    "    for fips_code in fips_codes:\n",
    "\n",
    "        # fetch the subset of the tweet data frame corresponding to each unique FIPS code\n",
    "        sub_df_county = df_cleaned.loc[df_cleaned.FIPS == fips_code]\n",
    "        assert len(sub_df_county.FIPS.unique()) == 1\n",
    "\n",
    "        # save or else open and append\n",
    "        fname = \"{}.pkl\".format(fips_code)\n",
    "        fpath = os.path.join(out_dir, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            print(\"county file for fips code {} already exists.\".format(fips_code))\n",
    "\n",
    "            # open existing file and append tweets as new rows\n",
    "            existing_df_county = pd.read_pickle(fpath)\n",
    "\n",
    "            # this step could create new duplicates if ran twice for the same files.\n",
    "            # after ordering all tweets by county, we should run a drop-duplicates \n",
    "            # script BEFORE resampling to daily sentiment distributions.\n",
    "            merged_df_county = pd.concat([existing_df_county, sub_df_county], axis=0)\n",
    "\n",
    "            # drop potential duplicate rows based on unique tweet ID (ESSENTIAL STEP)\n",
    "            merged_df_county = merged_df_county.drop_duplicates(subset=\"ID\")        \n",
    "\n",
    "            # re-sort tweets by date\n",
    "            merged_df_county = merged_df_county.sort_values(\"Date\", ascending=True)\n",
    "            merged_df_county.to_pickle(fpath)\n",
    "\n",
    "            # number of new unique tweets appended\n",
    "            new_tweets = existing_df_county.shape[0] - merged_df_county.shape[0]\n",
    "            if new_tweets > 0:\n",
    "                print(\"appended {} new tweets to county with fips code {}\\n\".format(new_tweets, fips_code))\n",
    "            else:\n",
    "                print(\"appended no new tweets to county with fips code {}\\n\".format(fips_code))\n",
    "        else:\n",
    "            print(\"creating new file for county with fips code {}.\".format(fips_code))\n",
    "            sub_df_county.to_pickle(fpath)\n",
    "            \n",
    "# save summary of number of tweets dropped due to missing geolocation \n",
    "df_missing_geolocation = pd.DataFrame(missing_geolocation, index=[\"n_before\", \"n_after\", \"n_missing\", \"n_missing_ratio\"]).transpose()\n",
    "df_missing_geolocation.to_excel(\"/Users/felix/ETH/Shared/100/info_missing_geolocation_felix.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (bd4pp)",
   "language": "python",
   "name": "pycharm-3837c9a2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of sentiment scores before the resampling step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code sorts the raw tweets with the computed sentiment scores into county-level files based on their FIPS codes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Processing file 192.p ---------------\n",
      "Dropped 230 tweets due to missing geolocation.\n",
      "\n",
      "county file for fips code 06001 already exists.\n",
      "appended 0 new tweets to county with fips code 06001\n",
      "\n",
      "county file for fips code 06013 already exists.\n",
      "appended 0 new tweets to county with fips code 06013\n",
      "\n",
      "county file for fips code 06075 already exists.\n",
      "appended 0 new tweets to county with fips code 06075\n",
      "\n",
      "county file for fips code 06095 already exists.\n",
      "appended 0 new tweets to county with fips code 06095\n",
      "\n",
      "county file for fips code 06019 already exists.\n",
      "appended 0 new tweets to county with fips code 06019\n",
      "\n",
      "county file for fips code 06041 already exists.\n",
      "appended 0 new tweets to county with fips code 06041\n",
      "\n",
      "county file for fips code 06055 already exists.\n",
      "appended 0 new tweets to county with fips code 06055\n",
      "\n",
      "county file for fips code 06077 already exists.\n",
      "appended 0 new tweets to county with fips code 06077\n",
      "\n",
      "county file for fips code 06081 already exists.\n",
      "appended 0 new tweets to county with fips code 06081\n",
      "\n",
      "\n",
      "--------------- Processing file 158.p ---------------\n",
      "Dropped 0 tweets due to missing geolocation.\n",
      "\n",
      "county file for fips code 05119 already exists.\n",
      "appended 0 new tweets to county with fips code 05119\n",
      "\n",
      "county file for fips code 05077 already exists.\n",
      "appended 0 new tweets to county with fips code 05077\n",
      "\n",
      "county file for fips code 05107 already exists.\n",
      "appended 0 new tweets to county with fips code 05107\n",
      "\n",
      "county file for fips code 05001 already exists.\n",
      "appended 0 new tweets to county with fips code 05001\n",
      "\n",
      "county file for fips code 05095 already exists.\n",
      "appended 0 new tweets to county with fips code 05095\n",
      "\n",
      "county file for fips code 05117 already exists.\n",
      "appended 0 new tweets to county with fips code 05117\n",
      "\n",
      "county file for fips code 05123 already exists.\n",
      "appended 0 new tweets to county with fips code 05123\n",
      "\n",
      "county file for fips code 05085 already exists.\n",
      "appended 0 new tweets to county with fips code 05085\n",
      "\n",
      "county file for fips code 05147 already exists.\n",
      "appended 0 new tweets to county with fips code 05147\n",
      "\n",
      "\n",
      "--------------- Processing file 1.p ---------------\n",
      "Dropped 21 tweets due to missing geolocation.\n",
      "\n",
      "county file for fips code 01097 already exists.\n",
      "appended 0 new tweets to county with fips code 01097\n",
      "\n",
      "county file for fips code 12033 already exists.\n",
      "appended 0 new tweets to county with fips code 12033\n",
      "\n",
      "county file for fips code 01003 already exists.\n",
      "appended 0 new tweets to county with fips code 01003\n",
      "\n",
      "county file for fips code 01001 already exists.\n",
      "appended 0 new tweets to county with fips code 01001\n",
      "\n",
      "county file for fips code 12113 already exists.\n",
      "appended 0 new tweets to county with fips code 12113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_dir = \"/Users/felix/Downloads/geolocated\"\n",
    "out_dir = \"/Users/felix/Downloads/fips_sorted\"\n",
    "\n",
    "in_flist = os.listdir(in_dir)\n",
    "\n",
    "# here, on the very top we would have a loop over the raw files:\n",
    "for fname in in_flist:\n",
    "    print(\"\\n--------------- Processing file {} ---------------\".format(fname))\n",
    "\n",
    "    # read file, auto-convert dtypes and set date time index\n",
    "    df = pd.read_pickle(os.path.join(in_dir, fname))\n",
    "    df = df.infer_objects()\n",
    "    df = df.set_index(\"Date\")\n",
    "\n",
    "    # TODO: It seems that the geolocation script did not locate all tweets. \n",
    "    # have to deal with those somehow. Drop them for now to experiment further.\n",
    "    df_cleaned = df.drop(df.loc[df.FIPS == \"\"].index, axis=0)\n",
    "    print(\"Dropped {} tweets due to missing geolocation.\\n\".format(df.shape[0] - df_cleaned.shape[0]))\n",
    "\n",
    "    # get unique fips codes within the raw file\n",
    "    fips_codes = df_cleaned.FIPS.unique()\n",
    "\n",
    "    # iterate over fips codes and store the corresponding tweets in seperate, county-level data frames\n",
    "    for fips_code in fips_codes:\n",
    "\n",
    "        # fetch the subset of the tweet data frame corresponding to each unique FIPS code\n",
    "        sub_df_county = df_cleaned.loc[df_cleaned.FIPS == fips_code]\n",
    "        assert len(sub_df_county.FIPS.unique()) == 1\n",
    "\n",
    "        # save or else open and append\n",
    "        fname = \"{}.pkl\".format(fips_code)\n",
    "        fpath = os.path.join(out_dir, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            print(\"county file for fips code {} already exists.\".format(fips_code))\n",
    "\n",
    "            # open existing file and append tweets as new rows\n",
    "            existing_df_county = pd.read_pickle(fpath)\n",
    "\n",
    "            # this step could create new duplicates if ran twice for the same files.\n",
    "            # after ordering all tweets by county, we should run a drop-duplicates \n",
    "            # script BEFORE resampling to daily sentiment distributions.\n",
    "            merged_df_county = pd.concat([existing_df_county, sub_df_county], axis=0)\n",
    "\n",
    "            # drop potential duplicate rows based on unique tweet ID (ESSENTIAL STEP)\n",
    "            merged_df_county = merged_df_county.drop_duplicates(subset=\"ID\")        \n",
    "\n",
    "            # re-sort tweets by date\n",
    "            merged_df_county = merged_df_county.sort_values(\"Date\", ascending=True)\n",
    "            merged_df_county.to_pickle(fpath)\n",
    "\n",
    "            # number of new unique tweets appended\n",
    "            new_tweets = existing_df_county.shape[0] - merged_df_county.shape[0]\n",
    "            print(\"appended {} new tweets to county with fips code {}\\n\".format(new_tweets, fips_code))\n",
    "\n",
    "        else:\n",
    "            print(\"creating new file for county with fips code {}.\".format(fips_code))\n",
    "            sub_df_county.to_pickle(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code calculates quantiles, mean and sd while resampling to a daily sampling. It should be applied once we have created the ordered, county-level tweet files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Resampling file 06019.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 05077.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 05117.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 05107.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 01003.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 01001.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 12033.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 05001.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 06041.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 06055.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 06095.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 06081.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 06075.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 01097.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 05147.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 06077.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 05095.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 06013.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 05123.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 12113.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 05085.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 06001.pkl ---------------\n",
      "\n",
      "--------------- Resampling file 05119.pkl ---------------\n"
     ]
    }
   ],
   "source": [
    "in_dir = \"/Users/felix/Downloads/fips_sorted\"\n",
    "out_dir = \"/Users/felix/Downloads/processed_daily\"\n",
    "\n",
    "in_flist = os.listdir(in_dir)\n",
    "\n",
    "# here, on the very top we would have a loop over the raw files:\n",
    "for fname in in_flist:\n",
    "    print(\"\\n--------------- Resampling file {} ---------------\".format(fname))\n",
    "\n",
    "    # read file, auto-convert dtypes and set date time index\n",
    "    df = pd.read_pickle(os.path.join(in_dir, fname))\n",
    "\n",
    "    # copy\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # iterate over the sentiment score columns\n",
    "    dfs_sentiments = []\n",
    "    sentiment_cols = ['polarity', 'subjectivity', 'positive', 'negative', 'neutral']\n",
    "\n",
    "    for col in sentiment_cols:\n",
    "        # quantiles\n",
    "        df_q = df_out[col].resample(\"D\").quantile(q=[0., 0.025, 0.25, 0.5, 0.75, 0.095, 1.])\n",
    "        df_q.index = df_q.index.set_names([\"date\", \"quantile\"])\n",
    "        df_q = df_q.unstack()\n",
    "        new_col = [\"{}_{}\".format(col, q) for q in df_q.columns]\n",
    "        df_q = df_q.rename(columns=dict(zip(df_q.columns, new_col)))\n",
    "\n",
    "        # mean and sd\n",
    "        df_q[\"{}_mean\".format(col)] = df_out[col].resample(\"D\").mean()\n",
    "        df_q[\"{}_sd\".format(col)] = df_out[col].resample(\"D\").std()\n",
    "\n",
    "        # sum of retweets\n",
    "        dfs_sentiments.append(df_q)\n",
    "\n",
    "    # concatenate\n",
    "    df_merged_daily = pd.concat(dfs_sentiments, axis=1)\n",
    "\n",
    "    # add the daily sum of retweets\n",
    "    df_merged_daily[\"retweets_total\".format(col)] = df_out[\"Retweets\"].resample(\"D\").sum()\n",
    "    \n",
    "    # store daily resampled data to pickle file\n",
    "    df_merged_daily.to_pickle(os.path.join(out_dir, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A check a la std = 0 could be used to drop all columns that have zero variation over time. This is the case for several quantiles of the different sentiment scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (bd4pp)",
   "language": "python",
   "name": "pycharm-3837c9a2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
